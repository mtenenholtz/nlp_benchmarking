{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, logging, os, gc\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WANDB = False\n",
    "\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    os.environ['WANDB_PROJECT'] = 'imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "ckpt_base_dir = 'outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011121273040771484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea06ba78a6c41589adc9501174f42f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('imdb')\n",
    "train_tok_ds = ds['train'].map(\n",
    "    lambda x: tokenizer(x['text'], max_length=1024, truncation=True), \n",
    "    batched=True,\n",
    "    remove_columns='text'\n",
    ")\n",
    "test_tok_ds = ds['test'].map(\n",
    "    lambda x: tokenizer(x['text'], max_length=1024, truncation=True), \n",
    "    batched=True,\n",
    "    remove_columns='text'\n",
    ")\n",
    "train_tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in train_tok_ds['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.update({\n",
    "            'output_hidden_states': False,\n",
    "        })\n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.transformer = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.pooler = MeanPooling()\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
    "        output = self.transformer(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output = output.last_hidden_state\n",
    "\n",
    "        output = self.pooler(output, attention_mask)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        logits = self.out(output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits.squeeze(), labels.float())\n",
    "                                \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits.sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(eval_pred): \n",
    "    predictions, labels = eval_pred\n",
    "    predictions = (predictions > 0.5).astype(int)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,bs,accum = 3e-5,4,2\n",
    "wd,epochs = 0.01,1\n",
    "grad_checkpoint = False\n",
    "freeze_layers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2812' max='2812' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2812/2812 06:57, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.194979</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_trainer(dds):\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "    args = TrainingArguments(\n",
    "        f'{ckpt_base_dir}/{model_name}-{exp_name}-fold-{i}',\n",
    "        learning_rate=lr, warmup_ratio=0., \n",
    "        gradient_accumulation_steps=accum,\n",
    "        lr_scheduler_type='cosine', \n",
    "        fp16=True,\n",
    "        fp16_full_eval=True,\n",
    "        evaluation_strategy=\"epoch\", \n",
    "        logging_strategy='epoch',\n",
    "        per_device_train_batch_size=bs, \n",
    "        per_device_eval_batch_size=bs*2,\n",
    "        greater_is_better=True, \n",
    "        group_by_length=True,\n",
    "        num_train_epochs=epochs, \n",
    "        weight_decay=wd, \n",
    "        report_to='wandb' if USE_WANDB else 'none', \n",
    "        run_name=f'{model_name}/{exp_name}/fold-{i}', \n",
    "        save_strategy='no',\n",
    "        save_total_limit=1,\n",
    "        dataloader_num_workers=4,\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "    \n",
    "    model = TransformerModel(model_name)\n",
    "\n",
    "    if grad_checkpoint:\n",
    "        model.transformer.gradient_checkpointing_enable()\n",
    "    \n",
    "    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                   tokenizer=tokenizer, data_collator=collator, compute_metrics=accuracy_metric)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "idxs = np.arange(len(train_tok_ds))\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(cv.split(idxs, train_tok_ds['label'])):\n",
    "    # just using a single model\n",
    "    if i != 0: continue\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.init(\n",
    "            project=os.environ['WANDB_PROJECT'],\n",
    "            name=f'{model_name}/{exp_name}/fold-{i}',\n",
    "            group=f'{model_name}/{exp_name}',\n",
    "            save_code=True\n",
    "        )\n",
    "    dds = DatasetDict({\n",
    "        \"train\": train_tok_ds.select(train_idx), \n",
    "        \"test\": train_tok_ds.select(val_idx)\n",
    "    })\n",
    "\n",
    "    trainer = get_trainer(dds)\n",
    "\n",
    "    trainer.train();\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9524"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_tok_ds).metrics['test_accuracy']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('kaggle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5768b04258121350f986a32a10c2b5b63ea833426012d4b5b8a887aeeef377c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
